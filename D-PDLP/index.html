<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="D-PDLP: Scaling PDLP to Distributed Multi-GPU Systems">
  <meta name="description" content="D-PDLP is the first distributed PDLP framework that extends PDHG to multi-GPU settings via 2D grid partitioning, achieving substantial speedups with strong scalability on massive-scale LP problems.">
  <meta name="keywords" content="Linear Programming, LP, PDHG, Primal-Dual Hybrid Gradient, Distributed Computing, Multi-GPU, GPU Acceleration, Optimization, cuPDLPx">
  <meta name="author" content="Hongpei Li, Yicheng Huang, Huikang Liu, Dongdong Ge, Yinyu Ye">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="D-PDLP Research Project">
  <meta property="og:title" content="D-PDLP: Scaling PDLP to Distributed Multi-GPU Systems">
  <meta property="og:description" content="D-PDLP is the first distributed PDLP framework that extends PDHG to multi-GPU settings via 2D grid partitioning, achieving substantial speedups with strong scalability on massive-scale LP problems.">
  <meta property="og:url" content="https://github.com/Lhongpei/D-PDLP">
  <meta property="og:image" content="https://github.com/Lhongpei/D-PDLP/static/images/comparison.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="D-PDLP - Distributed PDLP Framework">
  <meta property="article:published_time" content="2026-02-06T00:00:00.000Z">
  <meta property="article:author" content="Hongpei Li">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="Linear Programming">
  <meta property="article:tag" content="Distributed Computing">
  <meta property="article:tag" content="GPU Acceleration">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@D_PDLP">
  <meta name="twitter:creator" content="@D_PDLP">
  <meta name="twitter:title" content="D-PDLP: Scaling PDLP to Distributed Multi-GPU Systems">
  <meta name="twitter:description" content="D-PDLP is the first distributed PDLP framework that extends PDHG to multi-GPU settings via 2D grid partitioning, achieving substantial speedups with strong scalability on massive-scale LP problems.">
  <meta name="twitter:image" content="https://github.com/Lhongpei/D-PDLP/static/images/comparison.png">
  <meta name="twitter:image:alt" content="D-PDLP - Distributed PDLP Framework">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="D-PDLP: Scaling PDLP to Distributed Multi-GPU Systems">
  <meta name="citation_author" content="Li, Hongpei">
  <meta name="citation_author" content="Huang, Yicheng">
  <meta name="citation_author" content="Liu, Huikang">
  <meta name="citation_author" content="Ge, Dongdong">
  <meta name="citation_author" content="Ye, Yinyu">
  <meta name="citation_publication_date" content="2026">
  <meta name="citation_conference_title" content="arXiv">
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/2601.07628">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#0f172a">
  <meta name="msapplication-TileColor" content="#0f172a">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">

  <title>D-PDLP: Scaling PDLP to Distributed Multi-GPU Systems</title>
  
  <!-- Favicon -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;500;600;700&family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
  
  <!-- CSS -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="static/css/custom.css">
  
  <!-- Non-critical CSS -->
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript>
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Defer JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "D-PDLP: Scaling PDLP to Distributed Multi-GPU Systems",
    "description": "D-PDLP is the first distributed PDLP framework that extends PDHG to multi-GPU settings via 2D grid partitioning, achieving substantial speedups with strong scalability on massive-scale LP problems.",
    "author": [
      {"@type": "Person", "name": "Hongpei Li", "affiliation": {"@type": "Organization", "name": "Cardinal Operations"}},
      {"@type": "Person", "name": "Yicheng Huang", "affiliation": {"@type": "Organization", "name": "Shanghai University of Finance and Economics"}},
      {"@type": "Person", "name": "Huikang Liu", "affiliation": {"@type": "Organization", "name": "Shanghai Jiao Tong University"}},
      {"@type": "Person", "name": "Dongdong Ge", "affiliation": {"@type": "Organization", "name": "Shanghai Jiao Tong University"}},
      {"@type": "Person", "name": "Yinyu Ye", "affiliation": {"@type": "Organization", "name": "Stanford University"}}
    ],
    "datePublished": "2026-02-06",
    "publisher": {"@type": "Organization", "name": "arXiv"},
    "url": "https://github.com/Lhongpei/D-PDLP",
    "keywords": ["Linear Programming", "PDHG", "Distributed Computing", "Multi-GPU", "Optimization"]
  }
  </script>
</head>
<body class="d-pdlp-theme">

  <!-- Animated Background -->
  <div class="animated-bg">
    <div class="gradient-orb orb-1"></div>
    <div class="gradient-orb orb-2"></div>
    <div class="gradient-orb orb-3"></div>
  </div>

  <!-- Navigation -->
  <nav class="main-nav">
    <div class="container">
      <div class="nav-brand">
        <span class="logo-text">D-PDLP</span>
        <!-- <span class="logo-badge">arXiv</span> -->
      </div>
      <div class="nav-links">
        <a href="/research/" class="nav-link"><i class="fas fa-arrow-left"></i> Research</a>
        <a href="#abstract" class="nav-link">Abstract</a>
        <a href="#features" class="nav-link">Features</a>
        <a href="#performance" class="nav-link">Performance</a>
        <a href="https://github.com/Lhongpei/D-PDLP" target="_blank" class="nav-link github-link">
          <i class="fab fa-github"></i> GitHub
        </a>
      </div>
    </div>
  </nav>

  <!-- Hero Section -->
  <section class="hero-section">
    <div class="hero-content">
      <div class="container">
        <div class="hero-badge">
          <i class="fas fa-bolt"></i> First Distributed PDHG Solver
        </div>
        <h1 class="hero-title">
          <span class="gradient-text">D-PDLP</span>
        </h1>
        <p class="hero-subtitle">Scaling PDLP to Distributed Multi-GPU Systems</p>
        <p class="hero-description">
          Solve massive-scale linear programming problems with billions of variables 
          across multiple GPUs. Achieve <span class="highlight">near-linear speedup</span> 
          with full FP64 numerical accuracy.
        </p>
        
        <div class="hero-stats">
          <div class="stat-item">
            <span class="stat-number">6×</span>
            <span class="stat-label">Speedup on 8 GPUs</span>
          </div>
          <div class="stat-item">
            <span class="stat-number">10⁹+</span>
            <span class="stat-label">Variables Supported</span>
          </div>
          <div class="stat-item">
            <span class="stat-number">FP64</span>
            <span class="stat-label">Full Precision</span>
          </div>
        </div>

        <div class="hero-actions">
          <a href="https://arxiv.org/pdf/2601.07628" target="_blank" class="btn btn-primary">
            <i class="fas fa-file-pdf"></i> Read Paper
          </a>
          <a href="https://github.com/Lhongpei/D-PDLP" target="_blank" class="btn btn-secondary">
            <i class="fab fa-github"></i> View on GitHub
          </a>
        </div>

        <div class="hero-authors">
          <p>
            <a href="mailto:ishongpeili@gmail.com" class="author-link">Hongpei Li</a><sup>1</sup> • 
            <a href="#" class="author-link">Yicheng Huang</a><sup>2</sup> • 
            <a href="mailto:hkl1u@sjtu.edu.cn" class="author-link">Huikang Liu</a><sup>3</sup> • 
            <a href="mailto:ddge@sjtu.edu.cn" class="author-link">Dongdong Ge</a><sup>3</sup> • 
            <a href="#" class="author-link">Yinyu Ye</a><sup>4</sup>
          </p>
          <p class="affiliations">
            <sup>1</sup>Cardinal Operations &nbsp;|&nbsp; 
            <sup>2</sup>Shanghai University of Finance and Economics &nbsp;|&nbsp; 
            <sup>3</sup>Shanghai Jiao Tong University &nbsp;|&nbsp; 
            <sup>4</sup>Stanford University
          </p>
        </div>
      </div>
    </div>
    
    <div class="hero-visual">
      <img src="static/images/comparison.png" alt="D-PDLP Framework" class="hero-image">
    </div>
  </section>

  <!-- Open Source Banner -->
  <section class="opensource-banner">
    <div class="container">
      <div class="opensource-content">
        <div class="opensource-icon">
          <i class="fab fa-github"></i>
        </div>
        <div class="opensource-text">
          <h3>Open Source & Ready to Use</h3>
          <p>D-PDLP is freely available on GitHub. We welcome contributions, bug reports, and feature requests from the community!</p>
        </div>
        <a href="https://github.com/Lhongpei/D-PDLP" target="_blank" class="btn btn-github">
          <i class="fab fa-github"></i> Star on GitHub
        </a>
      </div>
    </div>
  </section>

  <!-- Abstract Section -->
  <section id="abstract" class="section abstract-section">
    <div class="container">
      <div class="section-header">
        <span class="section-tag">Overview</span>
        <h2 class="section-title">Abstract</h2>
      </div>
      <div class="abstract-content">
        <div class="abstract-card">
          <p>
            We present a <strong> distributed framework of the Primal-Dual Hybrid Gradient (PDHG)</strong> algorithm 
            for solving massive-scale linear programming (LP) problems. Although PDHG-based solvers demonstrate 
            strong performance on single-node GPU architectures, their applicability to industrial-scale instances 
            is often limited by single-GPU computational throughput.
          </p>
          <p>
            To overcome these challenges, we propose <span class="accent-text">D-PDLP</span>, the first 
            <strong>distributed PDLP framework</strong>, which extends PDHG to a multi-GPU setting via a practical 
            two-dimensional grid partitioning of the constraint matrix. We introduce a 
            <strong>block-wise random permutation</strong> strategy combined with 
            <strong>nonzero-aware matrix partitioning</strong> to improve load balance and computational efficiency.
          </p>
          <p>
            Extensive experiments on standard LP benchmarks (MIPLIB and Mittelmann) as well as huge-scale 
            real-world datasets show that our distributed implementation achieves 
            <span class="highlight-text">strong scalability</span> and 
            <span class="highlight-text">high performance</span> while preserving 
            <span class="highlight-text">full FP64 numerical accuracy</span>.
          </p>
        </div>
      </div>
    </div>
  </section>

  <!-- Key Features -->
  <section id="features" class="section features-section">
    <div class="container">
      <div class="section-header centered">
        <span class="section-tag">Key Features</span>
        <h2 class="section-title">What Makes D-PDLP Special</h2>
      </div>
      
      <div class="features-grid">
        <div class="feature-card featured">
          <div class="feature-icon">
            <i class="fas fa-network-wired"></i>
          </div>
          <h3>Distributed PDHG</h3>
          <p>The first distributed PDHG-based solver that scales across multiple GPUs via efficient 2D grid partitioning.</p>
          <span class="feature-badge">First</span>
        </div>
        
        <div class="feature-card">
          <div class="feature-icon icon-blue">
            <i class="fas fa-th"></i>
          </div>
          <h3>2D Grid Partitioning</h3>
          <p>Practical decomposition of constraint matrix across GPU clusters with minimal communication overhead.</p>
        </div>
        
        <div class="feature-card">
          <div class="feature-icon icon-purple">
            <i class="fas fa-random"></i>
          </div>
          <h3>Block-Wise Permutation</h3>
          <p>Novel random shuffling strategy that balances load while preserving local structure for efficient SpMV.</p>
        </div>
        
        <div class="feature-card">
          <div class="feature-icon icon-green">
            <i class="fas fa-code-branch"></i>
          </div>
          <h3>Open-sourced Software</h3>
          <p>Both Julia (prototyping) and C/CUDA (production) versions available. C version is open-sourced.</p>
        </div>
        
        <div class="feature-card">
          <div class="feature-icon icon-orange">
            <i class="fas fa-bolt"></i>
          </div>
          <h3>Near-Linear Speedup</h3>
          <p>Achieve up to 6× speedup on 8 GPUs with strong scalability on computationally intensive problems.</p>
        </div>
        
        <div class="feature-card">
          <div class="feature-icon icon-red">
            <i class="fas fa-bullseye"></i>
          </div>
          <h3>Full FP64 Accuracy</h3>
          <p>Maintains full double-precision numerical accuracy while delivering massive performance gains.</p>
        </div>
      </div>
    </div>
  </section>

  <!-- Architecture Visualization -->
  <section class="section architecture-section">
    <div class="container">
      <div class="section-header centered">
        <span class="section-tag">Architecture</span>
        <h2 class="section-title">How It Works</h2>
      </div>
      
      <div class="architecture-grid">
        <div class="arch-card">
          <img src="static/images/block.png" alt="2D Grid Partitioning">
          <div class="arch-content">
            <h3>2D Grid Partitioning</h3>
            <p>The constraint matrix is partitioned into blocks distributed across a 2D device mesh. Each device stores local blocks and executes partial SpMV operations before global synchronization.</p>
          </div>
        </div>
        
        <div class="arch-card">
          <img src="static/images/comparison.png" alt="Permutation Strategies">
          <div class="arch-content">
            <h3>Smart Load Balancing</h3>
            <p>Block-wise random permutation balances workload while preserving dense micro-structures, unlike naive approaches that cause imbalance or destroy locality.</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Performance Section -->
  <section id="performance" class="section performance-section">
    <div class="container">
      <div class="section-header centered">
        <span class="section-tag">Results</span>
        <h2 class="section-title">Scalability & Performance</h2>
      </div>

      <!-- Speedup Showcase -->
      <div class="speedup-showcase">
        <div class="speedup-card">
          <div class="speedup-header">
            <h3>Mega-Scale Speedups</h3>
            <p>Running time comparison on massive instances</p>
          </div>
          <div class="speedup-grid">
            <div class="speedup-item">
              <span class="speedup-name">zib03</span>
              <span class="speedup-dims">19M × 29M</span>
              <div class="speedup-bar">
                <div class="bar-single" style="width: 100%">812s</div>
              </div>
              <div class="speedup-bar">
                <div class="bar-multi" style="width: 31%">245s</div>
              </div>
              <span class="speedup-factor">3.3× faster</span>
            </div>
            
            <div class="speedup-item">
              <span class="speedup-name">mcf_2500</span>
              <span class="speedup-dims">1.5M × 126M</span>
              <div class="speedup-bar">
                <div class="bar-single" style="width: 100%">2943s</div>
              </div>
              <div class="speedup-bar">
                <div class="bar-multi" style="width: 18%">504s</div>
              </div>
              <span class="speedup-factor">5.8× faster</span>
            </div>
            
            <div class="speedup-item">
              <span class="speedup-name">sdm_50k</span>
              <span class="speedup-dims">5.5M × 10M</span>
              <div class="speedup-bar">
                <div class="bar-single" style="width: 100%">377s</div>
              </div>
              <div class="speedup-bar">
                <div class="bar-multi" style="width: 17%">61s</div>
              </div>
              <span class="speedup-factor">6.2× faster</span>
            </div>
          </div>
          <div class="speedup-legend">
            <span><span class="legend-dot single"></span> 1 GPU</span>
            <span><span class="legend-dot multi"></span> 8 GPUs</span>
          </div>
        </div>
      </div>

      <!-- Julia vs C Table -->
      <div class="comparison-section">
        <h3 class="comparison-title">
          <i class="fas fa-code"></i> Julia vs C Implementation
        </h3>
        <div class="table-wrapper">
          <table class="comparison-table">
            <thead>
              <tr>
                <th>Instance</th>
                <th class="text-center" colspan="2">4 GPUs</th>
                <th class="text-center" colspan="2">8 GPUs</th>
              </tr>
              <tr class="subheader">
                <th></th>
                <th class="text-right">Julia</th>
                <th class="text-right highlight">C</th>
                <th class="text-right">Julia</th>
                <th class="text-right highlight">C</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><strong>zib03</strong> <span class="row-tag">Koch</span></td>
                <td class="text-right">336s</td>
                <td class="text-right best">283s</td>
                <td class="text-right">245s</td>
                <td class="text-right best">205s</td>
              </tr>
              <tr>
                <td><strong>rand-10m</strong> <span class="row-tag">PageRank</span></td>
                <td class="text-right">5s</td>
                <td class="text-right best">3s</td>
                <td class="text-right">4s</td>
                <td class="text-right best">2s</td>
              </tr>
              <tr>
                <td><strong>mcf_2500</strong> <span class="row-tag">MCF</span></td>
                <td class="text-right">935s</td>
                <td class="text-right best">836s</td>
                <td class="text-right">504s</td>
                <td class="text-right best">435s</td>
              </tr>
              <tr>
                <td><strong>wil50</strong> <span class="row-tag">QAP</span></td>
                <td class="text-right">31s</td>
                <td class="text-right best">29s</td>
                <td class="text-right">25s</td>
                <td class="text-right best">22s</td>
              </tr>
              <tr>
                <td><strong>tai50b</strong> <span class="row-tag">QAP</span></td>
                <td class="text-right">52s</td>
                <td class="text-right best">43s</td>
                <td class="text-right">43s</td>
                <td class="text-right best">35s</td>
              </tr>
              <tr class="highlight-row">
                <td><strong>ds1</strong> <span class="row-tag">Unit Commitment</span></td>
                <td class="text-right">143s</td>
                <td class="text-right best">42s</td>
                <td class="text-right">122s</td>
                <td class="text-right best">31s</td>
              </tr>
              <tr>
                <td><strong>ds2</strong> <span class="row-tag">Unit Commitment</span></td>
                <td class="text-right">691s</td>
                <td class="text-right best">397s</td>
                <td class="text-right">591s</td>
                <td class="text-right best">288s</td>
              </tr>
              <tr class="summary-row">
                <td><strong>SGM10 Average</strong></td>
                <td class="text-right">101.2s</td>
                <td class="text-right best">86.1s <span class="speedup-tag">1.17×</span></td>
                <td class="text-right">77.7s</td>
                <td class="text-right best">62.2s <span class="speedup-tag">1.25×</span></td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="comparison-note">
          <i class="fas fa-info-circle"></i>
          The C-based solver is open-sourced on GitHub.
        </div>
      </div>
    </div>
  </section>

  <!-- Benchmarks -->
  <section class="section benchmarks-section">
    <div class="container">
      <div class="section-header centered">
        <span class="section-tag">Datasets</span>
        <h2 class="section-title">Benchmark Coverage</h2>
      </div>
      <div class="benchmarks-cloud">
        <span class="benchmark-tag size-lg">MIPLIB 2017</span>
        <span class="benchmark-tag size-lg">Mittelmann</span>
        <span class="benchmark-tag size-lg">PageRank</span>
        <span class="benchmark-tag size-lg">Multicommodity Flow</span>
        <span class="benchmark-tag size-lg">QAP Relaxations</span>
        <span class="benchmark-tag size-lg">Unit Commitment</span>
        <span class="benchmark-tag size-lg">Design Matching</span>
        <span class="benchmark-tag size-lg">zib03</span>
      </div>
    </div>
  </section>

  <!-- CTA Section -->
  <section class="section cta-section">
    <div class="container">
      <div class="cta-card">
        <h2>Ready to Accelerate Your LP Solving?</h2>
        <p>Get started with D-PDLP today and experience massive speedups on your large-scale optimization problems.</p>
        <div class="cta-buttons">
          <a href="https://github.com/Lhongpei/D-PDLP" target="_blank" class="btn btn-large btn-primary">
            <i class="fab fa-github"></i> Get Started
          </a>
          <a href="https://arxiv.org/pdf/2601.07628" target="_blank" class="btn btn-large btn-secondary">
            <i class="fas fa-file-pdf"></i> Read the Paper
          </a>
        </div>
      </div>
    </div>
  </section>

  <!-- BibTeX -->
  <section class="section bibtex-section">
    <div class="container">
      <div class="section-header centered">
        <span class="section-tag">Citation</span>
        <h2 class="section-title">BibTeX</h2>
      </div>
      <div class="bibtex-card">
        <pre><code>@misc{li2026singlegpuscalingpdlpdistributed,
      title={Beyond Single-GPU: Scaling PDLP to Distributed Multi-GPU Systems}, 
      author={Hongpei Li and Yicheng Huang and Huikang Liu and Dongdong Ge and Yinyu Ye},
      year={2026},
      eprint={2601.07628},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/2601.07628}, 
}</code></pre>
        <button class="copy-btn" onclick="copyBibTeX()">
          <i class="fas fa-copy"></i> Copy
        </button>
      </div>
    </div>
  </section>

  <!-- Footer -->
  <footer class="main-footer">
    <div class="container">
      <div class="footer-content">
        <div class="footer-brand">
          <span class="footer-logo">D-PDLP</span>
          <p>Scaling PDLP to Distributed Multi-GPU Systems</p>
        </div>
        <div class="footer-links">
          <a href="https://github.com/Lhongpei/D-PDLP" target="_blank"><i class="fab fa-github"></i></a>
          <a href="https://arxiv.org/pdf/2601.07628" target="_blank"><i class="fas fa-file-pdf"></i></a>
        </div>
      </div>
      <div class="footer-bottom">
        <p>
          Built with <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> 
          • Licensed under <a href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">CC BY-SA 4.0</a>
        </p>
      </div>
    </div>
  </footer>

  <!-- Scroll to Top -->
  <button class="scroll-top" onclick="scrollToTop()" title="Scroll to top">
    <i class="fas fa-arrow-up"></i>
  </button>

</body>
</html>
